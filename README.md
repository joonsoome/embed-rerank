# ğŸ”¥ Single Model Embedding & Reranking API

**Lightning-fast text embeddings and document reranking powered by Apple Silicon & MLX**

<div align="center">

[![Apple Silicon](https://img.shields.io/badge/Apple_Silicon-Ready-blue?logo=apple&logoColor=white)](https://developer.apple.com/silicon/)
[![MLX Framework](https://img.shields.io/badge/MLX-Optimized-green?logo=apple&logoColor=white)](https://ml-explore.github.io/mlx/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)

</div>

---

## âš¡ Why This Matters

Transform your text processing with **10x faster** embeddings and reranking on Apple Silicon. Drop-in replacement for OpenAI API and Hugging Face TEI with **zero code changes** required.

### ğŸ† Performance Comparison

| Operation | This API (MLX) | OpenAI API | Hugging Face TEI | Traditional |
|-----------|----------------|------------|------------------|-------------|
| **Embeddings** | `0.78ms` | `200ms+` | `15ms` | `35ms` |
| **Reranking** | `1.04ms` | `N/A` | `25ms` | `150ms` |
| **Model Loading** | `0.36s` | `N/A` | `3.2s` | `6.6s` |
| **Cost** | `$0` | `$0.02/1K` | `$0` | `$0` |

*Tested on Apple M4 Max - Your results may vary but will still be amazing!*

---

## ğŸ¯ Quick Start

```bash
# 1. Clone and setup
git clone https://github.com/joonsoo-me/embed-rerank.git
cd embed-rerank
python -m venv .venv && source .venv/bin/activate

# 2. Install and run
pip install -r requirements.txt
python -m uvicorn app.main:app --host 0.0.0.0 --port 9000

# 3. Test it works
curl http://localhost:9000/health/
```

ğŸ‰ **Done!** Visit [http://localhost:9000/docs](http://localhost:9000/docs) for interactive API documentation.

---

## ğŸŒ Three APIs, One Service

Your service speaks **three languages fluently** - use whichever fits your project:

| API Standard | Embedding Endpoint | Reranking Endpoint | Use Case |
|-------------|-------------------|-------------------|----------|
| **ğŸ  Native** | `/api/v1/embed` | `/api/v1/rerank` | New projects, full control |
| **ğŸ¤– OpenAI** | `/v1/embeddings` | - | Existing OpenAI code |
| **ğŸ”„ TEI** | `/embed` | `/rerank` | Hugging Face TEI replacement |

### OpenAI Compatible (Drop-in Replacement)

```python
import openai

# Just change the base_url - everything else stays the same!
client = openai.OpenAI(
    api_key="dummy-key",  # Not used, but required
    base_url="http://localhost:9000/v1"
)

response = client.embeddings.create(
    input=["Hello world", "Apple Silicon is fast!"],
    model="text-embedding-ada-002"
)

print(f"Generated {len(response.data)} embeddings")
# ğŸš€ 10x faster than OpenAI, same exact code!
```

### TEI Compatible (Hugging Face Replacement)

```bash
# Before: Hugging Face TEI
curl -X POST "https://api-inference.huggingface.co/models/..." 
     -H "Authorization: Bearer $HF_TOKEN" 
     -d '{"inputs": ["Hello world"]}'

# After: Your Apple MLX service  
curl -X POST "http://localhost:9000/embed" 
     -d '{"inputs": ["Hello world"], "truncate": true}'

# Same API, 10x faster, $0 cost! ğŸ¯
```

### Native API (Full Control)

```bash
# Embeddings
curl -X POST "http://localhost:9000/api/v1/embed/" 
     -H "Content-Type: application/json" 
     -d '{"texts": ["Apple Silicon", "MLX acceleration"]}'

# Reranking  
curl -X POST "http://localhost:9000/api/v1/rerank/" 
     -H "Content-Type: application/json" 
     -d '{
       "query": "machine learning",
       "passages": ["AI is cool", "Dogs are pets", "MLX is fast"]
     }'
```

---

## âš™ï¸ Configuration

Create `.env` file for customization:

```env
# Backend (auto-detects Apple Silicon)
BACKEND=auto
MODEL_NAME=mlx-community/Qwen3-Embedding-4B-4bit-DWQ

# Server
HOST=0.0.0.0
PORT=9000
```

---

## ğŸ§ª Testing

```bash
# Test all endpoints (comprehensive)
python tests/test_tei_comprehensive.py

# Or run pytest
pytest tests/ -v

# Quick health check
curl http://localhost:9000/health/
```

---

## ğŸš€ What You Get

- **âœ… Zero Code Changes**: Drop-in replacement for OpenAI API and TEI
- **âš¡ 10x Performance**: Apple MLX acceleration on Apple Silicon  
- **ğŸ’° Zero Costs**: No API fees, runs locally
- **ğŸ”’ Privacy**: Your data never leaves your machine
- **ğŸ¯ Three APIs**: Native, OpenAI, and TEI compatibility
- **ğŸ“Š Production Ready**: Health checks, monitoring, structured logging

# ï¿½ Single Model Embedding & Reranking API

<div align="center">
<strong>Lightning-fast local embeddings & reranking for Apple Silicon (MLX-first, OpenAI & TEI compatible)</strong>
<br/><br/>
<a href="https://developer.apple.com/silicon/"><img src="https://img.shields.io/badge/Apple_Silicon-Ready-blue?logo=apple&logoColor=white" /></a>
<a href="https://ml-explore.github.io/mlx/"><img src="https://img.shields.io/badge/MLX-Optimized-green?logo=apple&logoColor=white" /></a>
<a href="https://fastapi.tiangolo.com/"><img src="https://img.shields.io/badge/FastAPI-009688?logo=fastapi&logoColor=white" /></a>
</div>

---

## ğŸ“Œ TL;DR
One lightweight service. One quantized model. Three fully compatible APIs (Native / OpenAI / TEI). Subâ€‘millisecond embedding + reranking on Apple Silicon using MLX with automatic fallback to PyTorch (MPS) or CPU.
âœ… **OpenAI Migration**: "Same OpenAI SDK code, but local and lightning fast"  
---

## âš¡ Why It Matters

| Benefit | What It Means |
|---------|---------------|
| 10x+ speed | Local MLX inference: no network latency |
| Zero cost | No paid API calls â€“ everything stays on-device |
| Private by design | Your data never leaves your machine |
| True dropâ€‘in | Works with existing OpenAI SDK & TEI clients |
| Single model | Embedding + rerank = simpler deployment |

### ğŸ† Performance Snapshot (M4 Max)

| Metric | MLX | PyTorch (MPS) | OpenAI Remote* | Hugging Face TEI* |
|--------|-----|---------------|----------------|------------------|
| Embedding latency (2 texts) | 0.78 ms | 35 ms | 200 ms+ | 15 ms |
| Rerank latency (3 passages) | 1.04 ms | 150 ms | N/A | 25 ms |
| Model load (cached) | 0.36 s | 2.71 s | Network | 3.2 s |
| Cost / 1K embed | $0 | $0 | $0.02 | $0 |

*Remote APIs include network overhead.
---
> Numbers are indicative; adjust expectations for different hardware / batch sizes.
## ğŸ“„ License

MIT License - build amazing things with this code!
## ğŸš€ Quick Start
## ğŸ§  AI Operations Made Simple

# 1. Clone & setup

```python
# Drop-in replacement for OpenAI API - 10x faster with full control!
import openai
# 2. Install & run
client = openai.OpenAI(
    api_key="dummy-key",  # Not used, but required
    base_url="http://localhost:9000/v1"
# 3. Health check

# 1ï¸âƒ£ Basic compatibility (existing code works unchanged!)
response = client.embeddings.create(
Visit: Swagger UI â†’ http://localhost:9000/docs â€¢ ReDoc â†’ /redoc
    model="text-embedding-ada-002"
)

## ğŸŒ API Modes (Choose Your Interface)
response = client.embeddings.create(
| Mode | Embeddings | Rerank | Typical Use |
|------|------------|--------|-------------|
| Native | `/api/v1/embed/` | `/api/v1/rerank/` | New projects / full control |
| OpenAI Compatible | `/v1/embeddings` | (use native/TEI rerank) | Existing OpenAI SDK code |
| TEI Compatible | `/embed` | `/rerank` | Replace Hugging Face TEI |
    model="text-embedding-ada-002",
### OpenAI (Dropâ€‘in)
        "return_timing": True          # â±ï¸ Get performance metrics
```python
from openai import OpenAI
client = OpenAI(base_url="http://localhost:9000/v1", api_key="dummy")
resp = client.embeddings.create(
    input=["Hello world", "Apple Silicon is fast!"],
    model="text-embedding-ada-002"
)
print(len(resp.data), 'embeddings')
```
)
### TEI Compatible
if hasattr(response.usage, 'mlx_processing_time'):
```bash
curl -X POST http://localhost:9000/embed \
  -H "Content-Type: application/json" \
  -d '{"inputs": ["Hello world"], "truncate": true}'

### Rerank Documents  
### Native (Full Control)
curl -X POST "http://localhost:9000/api/v1/rerank/" \
  -H "Content-Type: application/json" \
```bash
# List models (OpenAI format)
curl http://localhost:9000/v1/models

curl -X POST "http://localhost:9000/v1/embeddings" \
  -H "Content-Type: application/json" \
     -d '{"query": "machine learning", "passages": ["AI is cool", "Dogs are pets", "MLX is fast"]}'
  -H "Content-Type: application/json" \
  -d '{
    "input": ["High performance text", "Apple MLX acceleration"],
    "model": "text-embedding-ada-002",
## ğŸ”§ Enhanced OpenAI Parameters

Add optional MLX tuning fields directly in request body (OpenAI embeddings call):

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| batch_size | int | 32 | 1â€“128 batching for throughput |
| normalize | bool | false | L2 normalize embeddings |
| backend_preference | str | auto | mlx | torch | auto |
| device_preference | str | auto | mps | cpu | auto |
| max_tokens_per_text | int | 512 | Truncate safeguard |
| return_timing | bool | false | Include performance metrics |

Example:
```python
resp = client.embeddings.create(
  input=texts,
  model="text-embedding-ada-002",
  extra_body={
    "batch_size": 64,
    "normalize": True,
    "backend_preference": "mlx",
    "return_timing": True
  }
)
```

Sample enhanced response (abridged):
```json
{
  "data": [...],
  "usage": {
    "prompt_tokens": 42,
    "total_tokens": 42,
    "mlx_processing_time": 0.0045,
    "total_processing_time": 0.0123,
    "backend_used": "MLXBackend",
    "device_used": "mps",
    "batch_size_used": 64
  }
}
```

---

## ğŸ”— Endpoint Reference

| Endpoint | Method | Mode | Purpose |
|----------|--------|------|---------|
| `/api/v1/embed/` | POST | Native | Embeddings |
| `/api/v1/rerank/` | POST | Native | Reranking |
| `/v1/embeddings` | POST | OpenAI | OpenAI-compatible embeddings |
| `/v1/models` | GET | OpenAI | List models |
| `/embed` | POST | TEI | TEI embeddings |
| `/rerank` | POST | TEI | TEI reranking |
| `/info` | GET | TEI | Service info |
| `/health/` | GET | System | Health & metrics |
| `/docs` | GET | System | Swagger UI |
| `/redoc` | GET | System | ReDoc docs |

---

## ğŸ›  Configuration (.env)
    "normalize": true,
```env
BACKEND=auto                                   # auto | mlx | torch
MODEL_NAME=mlx-community/Qwen3-Embedding-4B-4bit-DWQ
HOST=0.0.0.0
PORT=9000
BATCH_SIZE=32
MAX_TEXTS_PER_REQUEST=100
LOG_LEVEL=INFO
LOG_FORMAT=json
```
    "return_timing": true
Runtime precedence: request params > headers > env defaults.
  -H "Content-Type: application/json" \
---
```
## âœ¨ Key Features

| Category | Highlights |
|----------|------------|
| Performance | Subâ€‘ms embedding, ms-level reranking |
| Efficiency | 4â€‘bit quantized model, unified memory |
| Flexibility | Native + OpenAI + TEI in one binary |
| Resilience | Automatic backend fallback (MLX â†’ MPS â†’ CPU) |
| Observability | Structured logging + health metrics |
| Simplicity | Single model for both tasks |
| Privacy | 100% local â€“ no data egress |
  -H "Content-Type: application/json" \
---
# TEI reranking (10x faster than standard TEI)
## ğŸ“Š Benchmarks (Illustrative)

| Backend | Load (cached) | Throughput (texts/s) | Single Batch Latency | Memory (usage %) |
|---------|---------------|----------------------|----------------------|------------------|
| MLX | 0.36 s | 30,411 | 0.78 ms | 13.4% |
| PyTorch (MPS) | 2.71 s | 569 | 35 ms | 15.2% |

> First run downloads model (oneâ€‘time). Subsequent starts use cached weights.
    "query": "What is Apple MLX?",
    "texts": [
      "Apple MLX is a machine learning framework for Apple Silicon",
## ğŸ§ª Testing & Benchmark
      "MLX delivers incredible performance on Apple hardware"
```bash
# Full test suite
pytest tests/ -v

# With coverage
pytest tests/ -v --cov=app

# Quick backend smoke
python - <<'PY'
import asyncio
from app.backends.factory import BackendFactory
async def main():
  b = BackendFactory.create_backend('auto')
  await b.initialize()
  r = await b.embed_texts(['Hello Apple MLX!'])
  print('âœ…', len(r.vectors), 'embeddings in', f"{r.processing_time:.4f}s")
asyncio.run(main())
PY

# Performance micro benchmark
python -m app.utils.benchmark
```
curl http://localhost:9000/info
```

## ï¿½ Error Handling

| Code | Meaning | Typical Cause |
|------|---------|---------------|
| 200 | Success | â€“ |
| 400 | Bad request | Invalid parameter range (e.g. batch_size) |
| 422 | Validation error | Malformed JSON / schema mismatch |
| 503 | Backend not ready | Model still loading |
| 500 | Internal error | Unexpected failure |

Example:
```json
{ "error": "api_error", "detail": "batch_size must be between 1 and 128", "status_code": 400 }
```
- **ï¿½ TEI Compatible**: Drop-in replacement for Hugging Face Text Embeddings Inference
- **ï¿½ğŸ”§ Configurable Performance**: Control batch sizes, backends, devices, and normalization
- **ğŸ“Š Detailed Metrics**: Optional performance monitoring and timing information
## ğŸ— Architecture
- **ğŸš€ Apple MLX Native**: Built specifically for Apple Silicon's unified memory architecture
```
app/
â”œâ”€â”€ main.py              # FastAPI bootstrap
â”œâ”€â”€ backends/            # MLX + PyTorch abstraction
â”œâ”€â”€ services/            # Business logic (embedding, rerank)
â”œâ”€â”€ routers/             # API route definitions
â””â”€â”€ utils/               # Benchmark, device detection, logging
```
- **ğŸ“Š Production Ready**: Health checks, monitoring, structured logging
## ğŸ¤ Contributing

1. Fork
2. Branch: `feat/your-feature`
3. Tests: `pytest -v`
4. Ensure style & lint (black/ruff if configured)
5. PR with concise description & performance notes (if applicable)

---

## ğŸŒŸ Community & References
- MLX Community: https://huggingface.co/mlx-community
- Example models:
  - Qwen3-Embedding-0.6B / 4B / 8B (various quantizations)

> Star â­ the repo if this helped you build faster on Apple Silicon.

---

## ï¿½ License
MIT â€” see [LICENSE](LICENSE).

---

<div align="center"><strong>ğŸš€ Built for Apple Silicon â€¢ Powered by MLX â€¢ Production Ready</strong><br/>Native â€¢ OpenAI â€¢ TEI â€” One Service.</div>

